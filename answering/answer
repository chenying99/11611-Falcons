#!/usr/bin/python answer

import re
import os
import sys
import math
import logging
import operator
import unicodedata
from Commons import *
from heapq import nlargest
from Article import Article
from itertools import chain
from textblob import TextBlob
from collections import Counter
from CoreferenceResolver import *
from collections import defaultdict
from QuestionClassifier import QuestionClassifier


__author__ = 'avnishs, pbamotra'

WORD = re.compile(r'\w+')
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=VERBOSITY)
logger = logging.getLogger('')


def run_simulation():
    """
    Runs the answering system in simulation mode and prints evaluation metrics
    """
    qc = load_qclassifer_model()

    total_acc = 0.0
    count = 0
    for idx in range(1, 11):
        article = 'data/set2/a' + str(idx)
        article_path = './11611-Falcons/' + article + '.txt'

        process_question = lambda question: process_string(question)[0]
        questions, ground_truth_answers = get_qa_by_path(article)
        
        questions = map(process_question, questions)

        if len(ground_truth_answers) == 0:
            continue

        article_sentences = process_article_file(article_path)
        curr_article = Article(article_sentences)

        count += 1

        logger.debug('_' * 100)

        predicted_answers = generate_answers(questions, curr_article, qc)

        logger.debug('_' * 100)

        answering_acc = evaluate_answers(predicted_answers, ground_truth_answers)

        logger.debug('_' * 100)

        logger.debug('Accuracy = ' + str(answering_acc))

        logger.debug('\n')
        total_acc += answering_acc

    logger.debug('=' * 100)
    logger.debug('Average Accuracy = ' + str(total_acc / float(max(count,1))))


def run_production(argv):
    """
    Runs the answering system in production mode

    :param argv: input arguments - article.txt questions.txt
    """
    if len(argv) != 4:
        logger.error('Usage: ./answer article.txt questions.txt')
        sys.exit(1)

    article_file, question_file = argv[2], argv[3]

    if len(article_file.strip()) == 0 or len(question_file) == 0:
        logger.error('Invalid input: Please check article and question file path')
        sys.exit(1)

    article_sentences = process_article_file(article_file)
    curr_article = Article(article_sentences)

    questions = list()
    
    process_question = lambda question: process_string(question)[0]

    with open(question_file, 'r') as q_file:
        questions = map(process_question, map(str.strip, q_file.readlines()))
    
    if len(questions) == 0:
        logger.error('Empty question found. Please check questions file')
        sys.exit(1)

    qc = load_qclassifer_model()
    predicted_answers = generate_answers(questions, curr_article, qc)
    for answer in predicted_answers:
        print answer


def load_qclassifer_model():
    """
    Loads/trains question classifier
    Data set: Li and Roth QA data

    :return: trained question classifier
    """
    if USE_PICKLE:
        qc = QuestionClassifier(use_pickle=USE_PICKLE)
    else:
        with open('./11611-Falcons/data/qa_classification_tr.txt', 'r') as f:
            lines = f.readlines()
            qc = QuestionClassifier(use_pickle=USE_PICKLE)
            qc.train(lines)

        with open('./11611-Falcons/data/qa_classification_te.txt', 'r') as f:
            lines = f.readlines()
            acc, pred = qc.test(lines)
            logger.debug('Testing accuracy was ' + acc)
    return qc


def evaluate_answers(predicted_answers, ground_truth_answers):
    """
    Evaluates answers during simulation

    :param predicted_answers: list of predicted answers
    :param ground_truth_answers: gold-standard answers by humans
    :return: accuracy score
    """
    total_score = 0.0
    invalid_qs = 0
    i = 0.0
    for i in range(len(predicted_answers)):
        score = 0.0
        if len(ground_truth_answers[i]) == 0:
            invalid_qs += 1
            continue
        actual_answer = str(nlp(u"{}".format(process_text([ground_truth_answers[i]])[0])).text).lower()
        predicted_answer = predicted_answers[i].lower()
        if actual_answer in predicted_answer:
            score += 1.0
        else:
            ans_terms = actual_answer.split()
            count = 0
            for term in ans_terms:
                if term in predicted_answer:
                    count += 1

            if count/float(len(ans_terms)) >= 0.55:
                score += 1.0
        logger.debug(' '.join([str(score), " | ", str(actual_answer), " | ", str(predicted_answer)]))
        total_score += score

    return total_score / float(i + 1 - invalid_qs)


def edit_distance(s1,s2):
    """
    Finds the edit distance beween two sentences

    :param s1: string 1
    :param s2: string 2
    :return: edit distance between two sentences
    """
    if len(s1) > len(s2):
        s1,s2 = s2,s1
    distances = range(len(s1) + 1)
    for index2,char2 in enumerate(s2):
        newDistances = [index2+1]
        for index1,char1 in enumerate(s1):
            if char1 == char2:
                newDistances.append(distances[index1])
            else:
                newDistances.append(1 + min((distances[index1],
                                             distances[index1+1],
                                             newDistances[-1])))
        distances = newDistances
    return distances[-1]


def generate_answers(questions_list, article, qc):
    """
    Generate answers to the list of question from the given article

    :param questions_list: list of questions to be answered
    :param article: article to be searched for answers
    :param qc: pre-trained question classifer
    :return:
    """
    candidate_threshold = 10
    predicted_answers = list()

    for question in questions_list:
        q_tag = qc.predict(question)

        logger.debug(' '.join(['Ques.', question, ' | ', 'Classified as', q_tag]))

        _max = -sys.maxint - 1
        _max_ngram = -sys.maxint - 1

        predicted_answer = None
        processed_question = nlp(u"{}".format(question))

        res_bm25 = dict(bm25_ranker(article, question, 1.2, 0.75, 0, candidate_threshold))
        res_cos = dict(cos_similarity_ranker(article, question, candidate_threshold))

        max_score = res_bm25[max(res_bm25, key=lambda i: res_bm25[i])]

        alpha = 1  # controls the weightage on BM25 vs cosine similarity
        for ans, score in res_bm25.iteritems():
            res_bm25[ans] = (score / float(max_score)) * alpha
            if ans in res_cos:
                res_bm25[ans] += (res_cos[ans]) * (1 - alpha)

        res = sorted(res_bm25.items(), key=operator.itemgetter(1), reverse=True)

        logger.debug('') 
        logger.debug('Candidate answers')
        logger.debug('=' * 100)

        
        top_n = nlargest(3, res, key=operator.itemgetter(1))
        for (candidate_bm25, score_bm25) in top_n:
            processed_candidate = nlp(u'{}'.format(candidate_bm25))
            rank_points = score_bm25 + get_points(question, candidate_bm25)
            logger.debug(' '.join([candidate_bm25, str(score_bm25),str(rank_points)]))
            if rank_points > _max:
                predicted_answer = candidate_bm25
                _max = rank_points
                _max_ngram = rank_points

        if q_tag == 'YES_NO':
            predicted_answer = 'yes' if _max_ngram >= 0.5 else 'no'
        
        logger.debug(' '.join(['Answ.', str(predicted_answer)]))
        predicted_answers.append(predicted_answer)

    return predicted_answers


def create_ngrams(terms, n=2):
        """
        Creates n-grams from the terms
        :param terms: list of terms from which n-grams are to be formed
        :param n: length of the n-gram
        :return: list of n-grams
        """
        n_grams = map(list, zip(*[terms[i:] for i in range(n)]))
        return [' '.join(n_gram) for n_gram in n_grams]


#def get_points(rank, bm25_score, spacy_answer_candidate, spacy_question, q_tag, qc, question, candidate_bm25):
def get_points(question, candidate_bm25):
    """
    Used to rank final answer

    :param bm25_score: BM25 score
    :param spacy_answer_candidate: candidate answer as a spacy doc
    :param spacy_question: question as a spacy doc
    :param q_tag: tag generated by question classifier
    :return: score
    """
    ner_map = {'LOC': {'LOC', 'GPE'},
               'NUM': {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'},
               'DESC': {'ALL'},
               'HUM': {'PERSON'},
               'ENTY': {'NORP', 'FACILITY', 'ORG', 'PERSON'},
               'ABBR': {'ALL'},
               'YES_NO': {'ALL'}}

    '''
    ner_component = 0
    possible_tags = ner_map[q_tag]
    ner_tags = set([str(word.ent_type_) for word in spacy_answer_candidate if len(word.ent_type_) > 0])
    ner_score = sum(map(lambda tag: 1 if tag in possible_tags else 0, ner_tags))
    ner_component = 0.1 * (1 if ner_score > 0 else 0)
    '''

    answer_ed = process_string(candidate_bm25, remove_punct=True)[0].lower().split()
    question_ed = process_string(question, remove_punct=True)[0].lower().split()
    distance = edit_distance(answer_ed, question_ed) / float(len(answer_ed))

    # + ner_component #ngram_score / float((len(q_ngram) * len(a_ngram)) + 1) + (1 / float(distance))
    return 1 / float(distance) 


def get_qa_by_path(base_path):
    """
    Used for development purpose to read qa pairs for a particular article

    :param base_path: non-canonical path of article, Eg. data/setX/aX
    :return: qa pairs for article specified by base_path as a list
    """
    with open('./11611-Falcons/data/view_team_qnsans.txt') as t_file:
        questions_answers = dict()
        for row in t_file:
            r = row.split('\t')
            if r[3] == base_path:
                questions_answers[r[5]] = r[8]
    return questions_answers.keys(), questions_answers.values()


def process_string(input_string, remove_stop=False, remove_punct=False):
    """
    Used to process any string

    :param input_string: string to be processes
    :param remove_stop: Set true if stopwords are to be removed
    :param remove_punct: Set true if punctuations are to be removed
    :return: array(s) of processed string
    """
    cleaned = unicodedata.normalize('NFKD', input_string.decode('utf-8').strip()).encode('ASCII', 'ignore')
    text = nlp(u"{}".format(cleaned))
    result = None
    sentences = list(text.sents)

    if not remove_stop:
        result = [[token for token in spacy_cleaned] for spacy_cleaned in sentences]
    else:
        result = [[token for token in spacy_cleaned if not token.is_stop] for spacy_cleaned in sentences]

    if remove_punct:
        return [' '.join([str(token) for token in sentence if not token.is_punct]) for sentence in result]
 
    return [' '.join([str(token) for token in sentence]) for sentence in result]


def process_article_file(filename):
    """
    Process articles by removing irrelevant sentences and removing non-ascii characters

    :param filename: path of the article
    :return: processed article as a list of sentences
    """
    if os.stat(filename).st_size==0:
        logger.error('Found empty article file')
        sys.exit(1)

    result = list()
    with open(filename, 'r') as article:
        for idx, line in enumerate(article):
            cleaned = process_string(line)
            result.append(resolve_coreference(cleaned))
    sentences = filter(lambda sent: (len(sent.split()) > 5) and sent.strip().endswith('.'),
                       list(chain.from_iterable(result)))
    return sentences


def process_text(sentences):
    """
    Processes sentences to cleaner form

    :param sentences: list of sentence
    :return: processes texts
    """
    new_sen = list()
    for sen in sentences:
        s = re.sub('([.,!?\'\"()])', '', sen)
        s = s.strip()
        if len(s) < 1:
            continue
        new_sen.append(s)
    return new_sen


def edit_dist_ranker(article, question, k):
    """
    Returns list of ranked answers by edit distance

    :param article: article to be searched for
    :param question: question to be answered
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    question = process_string(question, remove_punct=True)[0].lower().split()
    line2score = defaultdict()

    for sentence in article.sentences:
        _sentence = process_string(sentence, remove_punct=True)[0].lower().split()
        score = len(_sentence) / float(edit_distance(question, _sentence))
        line2score[sentence] = score

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def bm25_ranker(article, question, k1, b, k3, k):
    """
    Returns list of ranked answers

    :param article: article to be searched for
    :param question: question to be answered
    :param k1: BM25 parameter
    :param b: BM25 parameter
    :param k3: BM25 parameter
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    df = defaultdict(int)
    line2score = defaultdict()
    question = question.lower()
    qtf = Counter(question.split())

    for term in qtf.keys():
        for sentence in article.sentences:
            sentence = sentence.lower()
            if term in sentence:
                df[term] = df.get(term, 0) + 1

        for sentence in article.sentences:
            _sentence = sentence
            sentence = sentence.lower()
            tf = Counter(sentence.split())
            if term in sentence:
                log_term = math.log10((article.get_num_sentences() - df[term] + 0.5) / float(df[term] + 0.5))
                k1_term = k1 * ((1 - b) + b * (len(sentence.split()) / float(article.get_avg_doclen())))
                k3_term = (float((k3 + 1) * qtf[term])) / (k3 + qtf[term])
                middle_term = float(tf[term]) / (tf[term] + k1_term)
                line2score[_sentence] = line2score.get(_sentence, 0) + log_term * middle_term * k3_term

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def cos_similarity_ranker(article, question, k):
    """
    Finds the cosine similarity between article sentences and question
    :param article: article to be searched for
    :param question: question to be answered
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    line2score = defaultdict()
    question = question.lower()
    for sentence in article.sentences:
        sentence = sentence.lower()
        line2score[sentence] = get_cosine(text_to_vector(question), text_to_vector(sentence))

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def get_cosine(vec1, vec2):
    """
    Finds the cosine between two vectors
    :param vec1: vector 1
    :param vec2: vector 2
    :return: return cosine similarity value
    """
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])
    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator


def text_to_vector(text):
    """
    Converts text vector containing counts

    :param text: text to be converted into a vector
    :return: returns vector of counts
    """
    words = WORD.findall(str(text))
    return Counter(words)


def main(argv=None):
    if argv is None:
        argv = sys.argv

    logger.debug('Booted Falcon answering system')

    if IS_PRODUCTION_MODE:
        run_production(argv)
    else:
        run_simulation()

    logger.debug('Terminating Falcon answering system')

if __name__ == '__main__':
    main()
