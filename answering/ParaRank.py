__author__ = 'avnishs'

import re
import sys
import math
import operator
import unicodedata
import numpy as np
from spacy.en import English
from Article import Article
from itertools import chain
from textblob import TextBlob
from collections import Counter
from collections import defaultdict
from QuestionClassifier import QuestionClassifier


WORD = re.compile(r'\w+')


def main(argv=None):
    if argv is None:
        argv = sys.argv

    print 'started'

    article = 'data/set2/a7'                # TODO: Should be handled as per requirement!
    article_path = '../' + article + '.txt'
    nlp = English()
    candidate_threshold = 10

    with open('../data/qa_classification_tr.txt', 'r') as f:
        lines = f.readlines()
        qc = QuestionClassifier(use_pickle=True)
        qc.train(lines)


    # with open('../data/qa_classification_te.txt', 'r') as f:
    #     lines = f.readlines()
    #     acc, pred = qc.test(lines)
    #     print 'Testing accuracy was', acc


    article_sentences = process_article_file(article_path)
    curr_article = Article(article_sentences)

    sample_qa = get_qa_by_path(article)

    for idx, qa in enumerate(sample_qa):
        if idx % 2 == 1:                    # TODO: Change this when submitting code
            continue

        question, answer = qa
        print '=' * 100
        q_tag = qc.predict(process_text([question])[0])

        print 'Ques.', question, '\t', 'Classified as', q_tag
        print 'Ans. ', answer

        print '-' * 100

        print 'Candidate answer sentences'

        _max = -sys.maxint - 1
        answer = None
        processed_q = nlp(u"{}".format(question))

        # TODO: Currently comparing results from both BM25 and cosine similarity, change this later!

        for ((idx_c, (candidate, score)), (idx_cos, (candidate2, score2))) \
                in zip(enumerate(bm25_ranker(curr_article, question, 1.2, 0.75, 0, candidate_threshold)),
                       enumerate(cos_similarity_ranker(curr_article, question, candidate_threshold))):
            rank_points = get_points(score, idx_c + 1, candidate_threshold, nlp(u'{}'.format(candidate)),
                                     processed_q, q_tag)

            print idx_c, candidate, score, rank_points
            # print idx_cos, candidate2, score2
            if rank_points > _max:
                answer = candidate
                _max = rank_points

        print 'Best answer -', answer

    print 'done!'


def get_points(bm25_score, rank, candidate_threshold, spacy_candidate, q, q_tag):
    """
    Used to rank final answer

    :param bm25_score: BM25 score
    :param rank: BM25 rank
    :param candidate_threshold: number of BM25 results
    :param spacy_candidate: candidate answer as a spacy doc
    :param q: question as a spacy doc
    :param q_tag: tag generated by question classifier
    :return: score
    """
    ner_map = {'LOC': {'LOC', 'GPE'},
               'NUM': {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'},
               'DESC': {'ALL'},
               'HUM': {'PERSON'},
               'ENTY': {'NORP', 'FACILITY', 'ORG'},
               'ABBR': {'ALL'}}

    possible_tags = ner_map[q_tag]

    ner_match_contribution = 1.25
    word_match_contribution = 1.75

    ner_score = sum(map(lambda tag: ner_match_contribution if tag in possible_tags else 0,
                        [str(word.ent_type_) for word in spacy_candidate]))

    answer_words = {str(_word.lemma_).strip() for _word in spacy_candidate}
    term_match = sum(map(lambda w: word_match_contribution if w in answer_words else 0,
                         [str(ques_word.lemma_) for ques_word in q]))

    # TODO: this formula needs tuning
    return (bm25_score / float(rank)) + ner_score + term_match


def get_qa_by_path(base_path):
    """
    Used for development purpose to read qa pairs for a particular article

    :param base_path: non-canonical path of article, Eg. data/setX/aX
    :return: qa pairs for article specified by base_path as a list
    """
    with open('../data/view_team_qnsans.txt') as t_file:
        questions_answers = list()
        for row in t_file:
            r = row.split('\t')
            if r[3] == base_path:
                questions_answers.append((r[5], r[8]))
    return questions_answers


def process_article_file(filename):
    """
    Process articles by removing irrelevant sentences and removing non-ascii characters

    :param filename: path of the article
    :return: processed article as a list of sentences
    """
    result = list()
    with open(filename, 'r') as article:
        for line in article:
            result.append(TextBlob(line.decode('utf-8')).sentences)

    sentences = filter(lambda sent: (len(sent.word_counts) > 5) and '.' in sent.tokens,
                       list(chain.from_iterable(result)))
    normalize_string = lambda sent: unicodedata.normalize('NFKD', sent.string.strip()).encode('ASCII', 'ignore')
    sentences = map(normalize_string, sentences)
    return sentences


def process_text(sentences):
    """
    Processes sentences to cleaner form

    :param sentences: list of sentence
    :return: processes texts
    """
    new_sen = list()
    for sen in sentences:
        s = re.sub('([.,!?\'\"()])', '', sen)
        s = s.strip()
        if len(s) < 1:
            continue
        new_sen.append(s)
    return new_sen


def bm25_ranker(article, question, k1, b, k3, k):
    """
    Returns list of ranked answers
    :param article: article to be searched for
    :param question: question to be answered
    :param k1: BM25 parameter
    :param b: BM25 parameter
    :param k3: BM25 parameter
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    df = defaultdict(int)
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    qtf = Counter(question.split())

    for term in qtf.keys():
        for sentence in article.sentences:
            if term in sentence:
                df[term] = df.get(term, 0) + 1

        for sentence in article.sentences:
            if term in sentence:
                log_term = math.log10((article.get_num_sentences() - df[term] + 0.5) / float(df[term] + 0.5))
                k1_term = k1 * ((1 - b) + b * (len(sentence) / float(article.get_avg_doclen())))
                k3_term = (float((k3 + 1) * qtf[term])) / (k3 + qtf[term])
                middle_term = float(article.get_term_freq(term)) / (article.get_term_freq(term) + k1_term)
                line2score[sentence] = line2score.get(sentence, 0) + log_term * middle_term * k3_term

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def cos_similarity_ranker(article, question, k):
    """
    Finds the cosine similarity between article sentences and question
    :param article: article to be searched for
    :param question: question to be answered
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    for sentence in article.sentences:
        line2score[sentence] = get_cosine(text_to_vector(question), text_to_vector(sentence))

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def get_cosine(vec1, vec2):
    """
    Finds the cosine between two vectors
    :param vec1: vector 1
    :param vec2: vector 2
    :return: return cosine similarity value
    """
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])
    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator


def text_to_vector(text):
    """
    Converts text vector containing counts

    :param text: text to be converted into a vector
    :return: returns vector of counts
    """
    words = WORD.findall(text)
    return Counter(words)


if __name__ == '__main__':
    main()
    # print process_article_file('../data/set1/a2.txt')