__author__ = 'avnishs'

import re
import sys
import math
import operator
import unicodedata
import numpy as np
from CoreferenceResolver import *
from spacy.en import English
from Article import Article
from itertools import chain
from textblob import TextBlob
from collections import Counter
from collections import defaultdict
from QuestionClassifier import QuestionClassifier


WORD = re.compile(r'\w+')


def main(argv=None):
    if argv is None:
        argv = sys.argv

    print 'Started'

    article = 'data/set2/a7'                # TODO: Should be handled as per requirement!
    article_path = '../' + article + '.txt'
    nlp = English()
    candidate_threshold = 10

    with open('../data/qa_classification_tr.txt', 'r') as f:
        lines = f.readlines()
        qc = QuestionClassifier(use_pickle=True)
        qc.train(lines)


    # with open('../data/qa_classification_te.txt', 'r') as f:
    #     lines = f.readlines()
    #     acc, pred = qc.test(lines)
    #     print 'Testing accuracy was', acc


    article_sentences = process_article_file(article_path, nlp)
    curr_article = Article(article_sentences)

    sample_qa = get_qa_by_path(article)
    answering_score = 0.0

    for idx, qa in enumerate(sample_qa):
        if idx % 2 == 1:                    # TODO: Change this when submitting code
            continue

        question, actual_answer = qa
        print '=' * 100
        q_tag = qc.predict(process_text([question])[0])
        ptext = process_text([actual_answer])[0]
        actual_answer = str(nlp(u"{}".format(ptext)).text)

        print 'Ques.', question, '\t', 'Classified as', q_tag
        print 'Ans. ', actual_answer

        print '-' * 100

        print 'Candidate answer sentences'

        _max = -sys.maxint - 1
        predicted_answer = None
        processed_q = nlp(u"{}".format(question))

        # TODO: Currently comparing results from both BM25 and cosine similarity, change this later!

        res_bm25 = dict(bm25_ranker(curr_article, question, 1.2, 0.75, 0, candidate_threshold))
        res_cos = dict(cos_similarity_ranker(curr_article, question, candidate_threshold))

        max_score = res_bm25[max(res_bm25, key=lambda i: res_bm25[i])]

        alpha = 8.0
        for ans, score in res_bm25.iteritems():
            res_bm25[ans] = (score)*alpha
            if ans in res_cos:
                res_bm25[ans] += (res_cos[ans])*(1-alpha)*float(max_score)

        res = sorted(res_bm25.items(), key=operator.itemgetter(1),reverse=True)

        rank = 1
        for (candidate_bm25, score_bm25) in res:

            rank_points = get_points(score_bm25, rank, candidate_threshold, nlp(u'{}'.format(candidate_bm25)),
                                     processed_q, q_tag)

            print rank, candidate_bm25, score_bm25, rank_points
            # print idx_cos, candidate2, score2
            if rank_points > _max:
                predicted_answer = candidate_bm25
                _max = rank_points

            rank += 1

        print 'Best answer -', predicted_answer

        score = 0.0
        if actual_answer in predicted_answer:
            score = 10.0
        else:
            ans_terms = actual_answer.split()
            denom = len(ans_terms)
            count = 0
            for term in ans_terms:
                if term in predicted_answer:
                    count+=1

            if count == denom:
                score = 10.0

        print score
        answering_score += score

    print 'accuracy = ' + str(answering_score/(idx*5))
    print 'done!'

def get_points(bm25_score, rank, candidate_threshold, spacy_candidate, q, q_tag):
    """
    Used to rank final answer
    :param bm25_score: BM25 score
    :param rank: BM25 rank
    :param candidate_threshold: number of BM25 results
    :param spacy_candidate: candidate answer as a spacy doc
    :param q: question as a spacy doc
    :param q_tag: tag generated by question classifier
    :return: score
    """
    ner_map = {'LOC': {'LOC', 'GPE'},
               'NUM': {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'},
               'DESC': {'ALL'},
               'HUM': {'PERSON'},
               'ENTY': {'NORP', 'FACILITY', 'ORG', 'PERSON'},
               'ABBR': {'ALL'}}

    possible_tags = ner_map[q_tag]
    ner_tags = [str(word.ent_type_) for word in spacy_candidate]
    ner_score = sum(map(lambda tag: 1 if tag in possible_tags else 0, ner_tags))

    # TODO: this formula needs tuning
    return (bm25_score / float(rank)) + 0.25*ner_score

def get_qa_by_path(base_path):
    """
    Used for development purpose to read qa pairs for a particular article

    :param base_path: non-canonical path of article, Eg. data/setX/aX
    :return: qa pairs for article specified by base_path as a list
    """
    with open('../data/view_team_qnsans.txt') as t_file:
        questions_answers = list()
        for row in t_file:
            r = row.split('\t')
            if r[3] == base_path:
                questions_answers.append((r[5], r[8]))
    return questions_answers

def process_article_file(filename, nlp):
    """
    Process articles by removing irrelevant sentences and removing non-ascii characters

    :param filename: path of the article
    :return: processed article as a list of sentences
    """
    result = list()
    with open(filename, 'r') as article:
        for line in article:
            cleaned = unicodedata.normalize('NFKD', line.decode('utf-8').strip()).encode('ASCII', 'ignore')
            result.append(TextBlob(resolve_coreference(cleaned, nlp)).sentences)

    sentences = filter(lambda sent: (len(sent.word_counts) > 5) and '.' in sent.tokens,
                       list(chain.from_iterable(result)))
    #normalize_string = lambda sent: unicodedata.normalize('NFKD', sent.string.strip()).encode('ASCII', 'ignore')
    #sentences = map(normalize_string, sentences)
    return sentences

def process_text(sentences):
    """
    Processes sentences to cleaner form
    :param sentences: list of sentence
    :return: processes texts
    """
    new_sen = list()
    for sen in sentences:
        s = re.sub('([.,!?\'\"()])', '', sen)
        s = s.strip()
        if len(s) < 1:
            continue
        new_sen.append(s)
    return new_sen

def bm25_ranker(article, question, k1, b, k3, k):
    """
    Returns list of ranked answers
    :param article: article to be searched for
    :param question: question to be answered
    :param k1: BM25 parameter
    :param b: BM25 parameter
    :param k3: BM25 parameter
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    df = defaultdict(int)
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    qtf = Counter(question.split())

    for term in qtf.keys():
        for sentence in article.sentences:
            if term in sentence:
                df[term] = df.get(term, 0) + 1

        for sentence in article.sentences:
            if term in sentence:
                log_term = math.log10((article.get_num_sentences() - df[term] + 0.5) / float(df[term] + 0.5))
                k1_term = k1 * ((1 - b) + b * (len(sentence) / float(article.get_avg_doclen())))
                k3_term = (float((k3 + 1) * qtf[term])) / (k3 + qtf[term])
                middle_term = float(article.get_term_freq(term)) / (article.get_term_freq(term) + k1_term)
                line2score[sentence] = line2score.get(sentence, 0) + log_term * middle_term * k3_term

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]

def cos_similarity_ranker(article, question, k):
    """
    Finds the cosine similarity between article sentences and question
    :param article: article to be searched for
    :param question: question to be answered
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    for sentence in article.sentences:
        line2score[sentence] = get_cosine(text_to_vector(question), text_to_vector(sentence))

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]

def get_cosine(vec1, vec2):
    """
    Finds the cosine between two vectors
    :param vec1: vector 1
    :param vec2: vector 2
    :return: return cosine similarity value
    """
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])
    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator

def text_to_vector(text):
    """
    Converts text vector containing counts

    :param text: text to be converted into a vector
    :return: returns vector of counts
    """
    words = WORD.findall(str(text))
    return Counter(words)

if __name__ == '__main__':
    main()
    # print process_article_file('../data/set1/a2.txt')