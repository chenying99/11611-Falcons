__author__ = 'avnishs'

import re
import sys
import math
import operator
import unicodedata
from CoreferenceResolver import *
from Commons import *
from Article import Article
from itertools import chain
from textblob import TextBlob
from collections import Counter
from collections import defaultdict
from QuestionClassifier import QuestionClassifier

WORD = re.compile(r'\w+')


def main(argv=None):
    if argv is None:
        argv = sys.argv

    print 'Started'

    with open('../data/qa_classification_tr.txt', 'r') as f:
        lines = f.readlines()
        qc = QuestionClassifier(use_pickle=True)
        qc.train(lines)

    # with open('../data/qa_classification_te.txt', 'r') as f:
    #     lines = f.readlines()
    #     acc, pred = qc.test(lines)
    #     print 'Testing accuracy was', acc

    total_acc = 0.0
    count = 0
    for idx in range(1, 11):
        article = 'data/set2/a' + `idx`  # TODO: Should be handled as per requirement!
        article_path = '../' + article + '.txt'

        print article

        questions, ground_truth_answers = get_qa_by_path(article)
        if len(ground_truth_answers) == 0:
            continue

        article_sentences = process_article_file(article_path, nlp)
        curr_article = Article(article_sentences)

        count += 1
        print '-' * 100
        predicted_answers = generate_answers(questions, curr_article, qc)
        print '-' * 100
        answering_acc = evaluate_answers(predicted_answers, ground_truth_answers)
        print '~' * 100
        print 'Accuracy = ' + str(answering_acc)
        total_acc += answering_acc

    print '=' * 100
    print 'Average Accuracy = ' + str(total_acc / float(count))
    print 'done!'


def evaluate_answers(predicted_answers, ground_truth_answers):
    total_score = 0.0
    i = 0.0
    for i in range(len(predicted_answers)):
        score = 0.0
        actual_answer = str(nlp(u"{}".format(process_text([ground_truth_answers[i]])[0])).text).lower()
        predicted_answer = predicted_answers[i].lower()
        if actual_answer in predicted_answer:
            score += 1.0
        else:
            ans_terms = actual_answer.split()
            count = 0
            for term in ans_terms:
                if term in predicted_answer:
                    count += 1

            if count == len(ans_terms):
                score += 1.0
        print str(score), " | ", actual_answer, " | ", predicted_answer
        total_score += score

    return total_score / float(i + 1)


def generate_answers(sample_qa, curr_article, qc):
    candidate_threshold = 10
    predicted_answers = list()

    for question in sample_qa:
        q_tag = qc.predict(process_text([question])[0])

        print 'Ques.', question, ' | ', 'Classified as', q_tag
        # print 'Ans. ', actual_answer

        _max = -sys.maxint - 1
        predicted_answer = None
        processed_q = nlp(u"{}".format(question))

        # TODO: Currently comparing results from both BM25 and cosine similarity, change this later!

        res_bm25 = dict(bm25_ranker(curr_article, question, 1.2, 0.75, 0, candidate_threshold))
        res_cos = dict(cos_similarity_ranker(curr_article, question, candidate_threshold))

        max_score = res_bm25[max(res_bm25, key=lambda i: res_bm25[i])]

        alpha = 1
        for ans, score in res_bm25.iteritems():
            res_bm25[ans] = (score / float(max_score)) * alpha
            if ans in res_cos:
                res_bm25[ans] += (res_cos[ans]) * (1 - alpha)

        res = sorted(res_bm25.items(), key=operator.itemgetter(1), reverse=True)

        rank = 1
        for (candidate_bm25, score_bm25) in res:

            rank_points = get_points(score_bm25, rank, candidate_threshold, nlp(u'{}'.format(candidate_bm25)),
                                     processed_q, q_tag)

            # print rank, candidate_bm25, score_bm25, rank_points
            # print idx_cos, candidate2, score2
            if rank_points > _max:
                predicted_answer = candidate_bm25
                _max = rank_points

            rank += 1

        # print 'Best answer -', predicted_answer
        predicted_answers.append(predicted_answer)

    return predicted_answers


def get_points(bm25_score, rank, candidate_threshold, spacy_candidate, q, q_tag):
    """
    Used to rank final answer
    :param bm25_score: BM25 score
    :param rank: BM25 rank
    :param candidate_threshold: number of BM25 results
    :param spacy_candidate: candidate answer as a spacy doc
    :param q: question as a spacy doc
    :param q_tag: tag generated by question classifier
    :return: score
    """
    ner_map = {'LOC': {'LOC', 'GPE'},
               'NUM': {'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'},
               'DESC': {'ALL'},
               'HUM': {'PERSON'},
               'ENTY': {'NORP', 'FACILITY', 'ORG', 'PERSON'},
               'ABBR': {'ALL'}}

    possible_tags = ner_map[q_tag]
    ner_tags = [str(word.ent_type_) for word in spacy_candidate]
    ner_score = sum(map(lambda tag: 1 if tag in possible_tags else 0, ner_tags))

    # TODO: this formula needs tuning
    return (bm25_score) + 0.25 * (ner_score / float(len(ner_tags)))


def get_qa_by_path(base_path):
    """
    Used for development purpose to read qa pairs for a particular article

    :param base_path: non-canonical path of article, Eg. data/setX/aX
    :return: qa pairs for article specified by base_path as a list
    """
    with open('../data/view_team_qnsans.txt') as t_file:
        questions_answers = dict()
        for row in t_file:
            r = row.split('\t')
            if r[3] == base_path:
                questions_answers[r[5]] = r[8]
    return questions_answers.keys(), questions_answers.values()


def process_article_file(filename, nlp):
    """
    Process articles by removing irrelevant sentences and removing non-ascii characters

    :param filename: path of the article
    :return: processed article as a list of sentences
    """
    result = list()
    with open(filename, 'r') as article:
        for line in article:
            cleaned = unicodedata.normalize('NFKD', line.decode('utf-8').strip()).encode('ASCII', 'ignore')
            result.append(TextBlob(resolve_coreference(cleaned, nlp)).sentences)

    sentences = filter(lambda sent: (len(sent.word_counts) > 5) and '.' in sent.tokens,
                       list(chain.from_iterable(result)))
    # normalize_string = lambda sent: unicodedata.normalize('NFKD', sent.string.strip()).encode('ASCII', 'ignore')
    # sentences = map(normalize_string, sentences)
    return sentences


def process_text(sentences):
    """
    Processes sentences to cleaner form
    :param sentences: list of sentence
    :return: processes texts
    """
    new_sen = list()
    for sen in sentences:
        s = re.sub('([.,!?\'\"()])', '', sen)
        s = s.strip()
        if len(s) < 1:
            continue
        new_sen.append(s)
    return new_sen


def bm25_ranker(article, question, k1, b, k3, k):
    """
    Returns list of ranked answers
    :param article: article to be searched for
    :param question: question to be answered
    :param k1: BM25 parameter
    :param b: BM25 parameter
    :param k3: BM25 parameter
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    df = defaultdict(int)
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    qtf = Counter(question.split())

    for term in qtf.keys():
        for sentence in article.sentences:
            if term in sentence:
                df[term] = df.get(term, 0) + 1

        for sentence in article.sentences:
            if term in sentence:
                log_term = math.log10((article.get_num_sentences() - df[term] + 0.5) / float(df[term] + 0.5))
                k1_term = k1 * ((1 - b) + b * (len(sentence) / float(article.get_avg_doclen())))
                k3_term = (float((k3 + 1) * qtf[term])) / (k3 + qtf[term])
                middle_term = float(article.get_term_freq(term)) / (article.get_term_freq(term) + k1_term)
                line2score[sentence] = line2score.get(sentence, 0) + log_term * middle_term * k3_term

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def cos_similarity_ranker(article, question, k):
    """
    Finds the cosine similarity between article sentences and question
    :param article: article to be searched for
    :param question: question to be answered
    :param k: Number of answers to be returned
    :return: list of ranked answers
    """
    pattern = re.compile('[\W_]+')
    pattern.sub('', question)
    line2score = defaultdict()
    for sentence in article.sentences:
        line2score[sentence] = get_cosine(text_to_vector(question), text_to_vector(sentence))

    return sorted(line2score.items(), key=operator.itemgetter(1), reverse=True)[0:min(k, len(line2score))]


def get_cosine(vec1, vec2):
    """
    Finds the cosine between two vectors
    :param vec1: vector 1
    :param vec2: vector 2
    :return: return cosine similarity value
    """
    intersection = set(vec1.keys()) & set(vec2.keys())
    numerator = sum([vec1[x] * vec2[x] for x in intersection])

    sum1 = sum([vec1[x] ** 2 for x in vec1.keys()])
    sum2 = sum([vec2[x] ** 2 for x in vec2.keys()])
    denominator = math.sqrt(sum1) * math.sqrt(sum2)

    if not denominator:
        return 0.0
    else:
        return float(numerator) / denominator


def text_to_vector(text):
    """
    Converts text vector containing counts

    :param text: text to be converted into a vector
    :return: returns vector of counts
    """
    words = WORD.findall(str(text))
    return Counter(words)


if __name__ == '__main__':
    main()
    # print process_article_file('../data/set1/a2.txt')
